{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 26, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "## Will use the English alphabet as my data\n",
    "inputs = np.array([\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
    "    [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
    "    [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
    "    [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
    "])\n",
    "\n",
    "expected = np.array([\n",
    "    [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"], \n",
    "    [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
    "    [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "])\n",
    "\n",
    "# However, this isnt good enough with how it is. We need to make the data usable\n",
    "\n",
    "def string_to_one_hot(inputs: np.ndarray) -> np.ndarray:\n",
    "    char_to_index = {char: i for i, char in enumerate(string.ascii_uppercase)}\n",
    "\n",
    "    one_hot_inputs = []\n",
    "    for row in inputs:\n",
    "        one_hot_list = []\n",
    "        for char in row:\n",
    "            if char.upper() in char_to_index:\n",
    "                one_hot_vector = np.zeros((len(string.ascii_uppercase), 1))\n",
    "                one_hot_vector[char_to_index[char.upper()]] = 1\n",
    "                one_hot_list.append(one_hot_vector)\n",
    "        one_hot_inputs.append(one_hot_list)\n",
    "\n",
    "    return np.array(one_hot_inputs)\n",
    "\n",
    "one_hot_outputTEST =  string_to_one_hot(expected)\n",
    "print(one_hot_outputTEST.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code structures my data so that the first element of each sequence (row) is processed simultaneously across all sequences. Since we have 5 sequences, the first letter from each of the 5 sequences is processed together in the first time step, followed by the second letter from each sequence in the next time step, and so on.\n",
    "\n",
    "Since we are using one-hot encoding, each letter is converted into a binary array of size 26, where all values are 0 except for a single 1 at the index corresponding to the letter's position in the alphabet. This transformation ensures that each letter is represented uniquely in a numerical format suitable for an RNN.\n",
    "\n",
    "***Why don't we just convert to ASCII? This excess storage seems overly complicated?***\n",
    "The reason for this excess storage is to reduce bias when processed through our RNN. Each letter needs to be treated equally. One-hot encoding solves this problem because it treats every letter independently, meaning there is no \"distance\" between letters unless the model learns it naturally.\n",
    "\n",
    "The storage is excessive and there are ways to deal with that such as word embeddings. However, lets stick to the tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Input Layer***\n",
    "- **inputs** = which is the sequential data in the form of numpy arrays.\n",
    "- **U** = is the weight of the matrix connecting input to the hidden layer\n",
    "- **delta_U** = is the gradient calculated during Back Propagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer:\n",
    "    inputs: np.ndarray\n",
    "    U: np.ndarray = None\n",
    "    delta_U: np.ndarray = None\n",
    "\n",
    "    def __init__(self, inputs: np.ndarray, hidden_size: int) -> None:\n",
    "        self.inputs = inputs\n",
    "        self.U = np.random.uniform(low=0, high=1, size=(hidden_size, len(inputs[0])))\n",
    "        self.delta_U = np.zeros_like(self.U)\n",
    "\n",
    "        def get_input(self, time_step: int) -> np.ndarray:\n",
    "            return self.inputs[time_step]\n",
    "        \n",
    "        def weighted_sum(self, time_step:int) -> np.ndarray:\n",
    "            return self.U @ self.get_input(time_step)\n",
    "        \n",
    "        def calculate_deltas_per_step(self, time_step, delta_weighted_sum: np.ndarray) -> None:\n",
    "            # (h_dimension, 1) @ (1, input_size) = (h_dimension, input_size)\n",
    "            self.delta_U += delta_weighted_sum @ self.get_input(time_step).T\n",
    "        \n",
    "        def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "            self.U -= learning_rate * self.delta_U\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Hidden Layer***\n",
    "- **States** = Stores activation of all time steps (internal memory of network)\n",
    "- **W** = Recurrent weight matrix\n",
    "- **delta_W** = gradient of W during BPTT\n",
    "- **bias** = b in the math formulas\n",
    "- **delta_bias** = gradient of b\n",
    "- **next_delta_activation** = stores the derivative of next steps loss function w.r.t. current activation, from this formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    states                : np.ndarray = None\n",
    "    W                     : np.ndarray = None\n",
    "    delta_W               : np.ndarray = None\n",
    "    bias                  : np.ndarray = None\n",
    "    delta_bias            : np.ndarray = None\n",
    "    next_delta_activation : np.ndarray = None\n",
    "\n",
    "    def __init__(self, vocab_size: int, size: int) -> None:\n",
    "        self.W                     = np.random.uniform(low=0, high=1, size=(size,size))\n",
    "        self.bias                  = np.random.uniform(low=0, high=1, size=(size,1))\n",
    "        self.states                = np.zeros(shape=(vocab_size, size, 1))\n",
    "        self.next_delta_activation = np.zeros(self.bias)\n",
    "        self.delta_bias            = np.zeros_like(self.bias)\n",
    "        self.delta_W               = np.zeros_like(self.W)\n",
    "    \n",
    "    def get_hidden_state(self, time_step: int) -> np.ndarray:\n",
    "        # If startin out at the beginning of seq, a[t-1] will return 0's\n",
    "        if time_step < 0:\n",
    "            return np.zeros_like(self.states[0])\n",
    "        return self.states[time_step]\n",
    "    \n",
    "    def set_hidden_state(self, time_step: int, hidden_state: np.ndarray) -> np.ndarray:\n",
    "        self.states[time_step] = hidden_state\n",
    "\n",
    "    def activate(self, weighted_input: np.ndarray, time_step: int) -> np.ndarray:\n",
    "        previous_hidden_state = self.get_hidden_state(time_step - 1)\n",
    "        # W @ h_prev => (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
    "        weighted_hidden_state = self.W @ previous_hidden_state\n",
    "        weighted_sum = weighted_input + weighted_hidden_state + self.bias\n",
    "        activation = np.tanh(weighted_sum) #(h_dimension, 1)\n",
    "        self.set_hidden_state(time_step, activation)\n",
    "        return activation\n",
    "\n",
    "    def calculate_deltas_per_step(self, time_step: int, delta_output: np.ndarray) -> np.ndarray:\n",
    "        delta_activation = delta_output + self.next_delta_activation\n",
    "        delta_weighted_sum = delta_activation * (1 - self.get_hidden_state(time_step) **2)\n",
    "        self.next_delta_activation = self.W.T @ delta_weighted_sum\n",
    "        self.delta_W += delta_weighted_sum @ self.get_hidden_state(time_step - 1).T\n",
    "        self.delta_bias += delta_weighted_sum\n",
    "        return delta_weighted_sum\n",
    "    \n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.W -= learning_rate * self.delta_W\n",
    "        self.bias -= learning_rate * self.delta_bias\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
